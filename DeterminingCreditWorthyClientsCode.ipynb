{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeterminingCreditWorthyClientsCode.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEI2CN5cQi0C"
      },
      "source": [
        "Code + Documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFccRwomQcfs"
      },
      "source": [
        "#Phase 1\n",
        "import pandas as pd\n",
        "#read data sets\n",
        "data_set1=pd.read_csv('/Users/josereyes/Documents/Data_Phase1/data/customer_data.csv')\n",
        "data_set2=pd.read_csv('/Users/josereyes/Documents/Data_Phase1/data/payment_data.csv')\n",
        "# change type of id Series elements in order to perform fancy indexing\n",
        "data_set1['id']=data_set1['id'].astype(str)\n",
        "data_set2['id']=data_set2['id'].astype(str)\n",
        "ids=list(data_set1['id'])\n",
        "# have id Series be the index\n",
        "data_set1=data_set1.set_index('id')\n",
        "data_set2=data_set2.set_index('id')\n",
        "#perform fancy indexing\n",
        "df=data_set2.loc[ids]\n",
        "df=df[~df.index.duplicated(keep='first')]\n",
        "#change index type and sort\n",
        "df.index=df.index.astype('int64')\n",
        "df= df.sort_index()\n",
        "data_set1.index=data_set1.index.astype('int64')\n",
        "data_set1= data_set1.sort_index()\n",
        "#join the two data frames\n",
        "df=data_set1.join(df)\n",
        "# Operation - aggregate number of times overdue for each type into one column to reduce the dimensionality of the data\n",
        "df['OVD']=df['OVD_t1']+df['OVD_t2']+df['OVD_t3']\n",
        "# Operation - One-hot encode OVD to indicate the presence of any overdue payments in order to make this column more pertinent to problem\n",
        "for i in range(len(df['OVD'])):\n",
        "    if df.iloc[i,-1]!=0:\n",
        "        df.iloc[i,-1]=1\n",
        "# remove columns prod_limit,update_date, and report_date, since the majority of values are NaN\n",
        "# remove OVD_t1, OVD_t2, OVD_t3 since this information is already captured in OVD\n",
        "df.drop('report_date', inplace=True, axis=1)\n",
        "df.drop('update_date', inplace=True, axis=1)\n",
        "df.drop('prod_limit', inplace=True, axis=1)\n",
        "df.drop('OVD_t1', inplace=True, axis=1)\n",
        "df.drop('OVD_t2', inplace=True, axis=1)\n",
        "df.drop('OVD_t3', inplace=True, axis=1)\n",
        "# Operation - fill NaN values to allow machine learning algorithm to make better predictions\n",
        "#I chose mean value because there was low variance in fea_2 feature\n",
        "df['fea_2']=df['fea_2'].fillna(df['fea_2'].mean())\n",
        "#I chose median value because there was high variance in highest_balance feature\n",
        "df['highest_balance']=df['highest_balance'].fillna(df['highest_balance'].median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQQS4hqlRCH5"
      },
      "source": [
        "#Phase 2\n",
        "# import cleaned dataset and display\n",
        "import pandas as pd\n",
        "df=pd.read_csv('data/DataFrame.csv')\n",
        "df.head(5)\n",
        "# display the first five rows data.head(5)\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#find which values in highest_balance are greater than 550000 and have bad risk\n",
        "mask=(df['highest_balance']>550000) & (df['label']==1)\n",
        "#find sum\n",
        "highrisk=mask.sum()\n",
        "#find which values in highest_balance are greater than 550000 and have good risk\n",
        "mask=(df['highest_balance']>550000) & (df['label']==0)\n",
        "#take sum\n",
        "lowrisk=mask.sum()\n",
        "#initialize list\n",
        "colors = []\n",
        "#replace 0 and 1 with the magenta or green for scatter plot\n",
        "for e in df['label']:\n",
        "    if e ==1:\n",
        "        colors.append('m')\n",
        "    else:\n",
        "        colors.append('g')\n",
        "#produce scatter plot\n",
        "plt.scatter(df['label'], df['highest_balance'], c=colors, alpha=.3)\n",
        "#label x axis\n",
        "plt.xlabel('label')\n",
        "#label y axis\n",
        "plt.ylabel('amount')\n",
        "#show plots\n",
        "plt.show()\n",
        "#drop new_balance\n",
        "df.drop('new_balance', inplace=True, axis=1)\n",
        "#slice data frame\n",
        "data=df.loc[:,'OVD_sum':'OVD'].copy()\n",
        "# make two-dimensional plot of each feature\n",
        "h=sns.PairGrid(data,palette='RdBu_r')\n",
        "h.map(plt.scatter, alpha=0.8)\n",
        "#Initialize dictionary\n",
        "DiversityofProducts={}\n",
        "#count how many of each product is contained in the data\n",
        "for element in df['prod_code']:\n",
        "    if element in DiversityofProducts:\n",
        "        DiversityofProducts[element]+=1\n",
        "    else:\n",
        "        DiversityofProducts[element]=1\n",
        "#convert to series\n",
        "s=pd.Series(DiversityofProducts)\n",
        "# plot\n",
        "plt.bar(x=s.index.to_numpy(), height=s.values)\n",
        "#label x axis\n",
        "plt.xlabel('Product Codes')\n",
        "#label y axis\n",
        "plt.ylabel('Count')\n",
        "# remove imbalance in class labels\n",
        "j=0\n",
        "while df.shape[0]>450:\n",
        "    if df['label'].iloc[j]==0:\n",
        "        df.drop(df.index[j], inplace=True, axis=0)\n",
        "    else:\n",
        "        j+=1\n",
        "# import relevant items\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#assign label values to a variable\n",
        "lab_values=df['label'].values\n",
        "#Sum the number of good risk class labels\n",
        "count0=np.equal(lab_values,0).sum()\n",
        "#Sum the number of bad risk credit labels\n",
        "count1=np.equal(lab_values,1).sum()\n",
        "#create plot\n",
        "plt.bar([1,2],height=[count0,count1])\n",
        "#create ticks on x axis\n",
        "plt.xticks([1,2],['-','+'])\n",
        "#create title\n",
        "plt.title('Class imbalance')\n",
        "#create y label\n",
        "plt.ylabel('Number of clients')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOy0yGuRSH5X"
      },
      "source": [
        "#Phase 3 \n",
        "# import your cleaned dataset \n",
        "from google.colab import files \n",
        "uploaded = files.upload() \n",
        "import pandas as pd \n",
        "#read data \n",
        "df = pd.read_csv('Data.csv') \n",
        "# display the first five rows data.head(5) \n",
        "df.head(5) \n",
        "#import relevant items \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "import sklearn.model_selection \n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "# copy data frame \n",
        "Z=df.copy().sample(frac=1,random_state=0) \n",
        "#normalization \n",
        "norm = MinMaxScaler().fit(Z) \n",
        "#transform the data \n",
        "Z = norm.transform(Z) \n",
        "#convert to data frame \n",
        "Z=pd.DataFrame(Z,index=df.index,columns=df.columns) \n",
        "#assign subset of data to X \n",
        "X=Z.loc[:,'fea_1':'fea_11'] \n",
        "#assign labels to y \n",
        "y=Z['label'] \n",
        "#import PCA \n",
        "from sklearn.decomposition import PCA \n",
        "pca=PCA(n_components=2) \n",
        "#fit PCA \n",
        "pca.fit(X) \n",
        "#perform dimensionality reduction \n",
        "X=pca.transform(X) \n",
        "#split the data \n",
        "Xtrain, Xtest, ytrain, ytest = sklearn.model_selection.train_test_split(X, y, train_size=0.7,random_state=0) \n",
        "#Instantiate knn classifier \n",
        "knn=KNeighborsClassifier(n_neighbors=7) \n",
        "#determine best accuracy \n",
        "Accuracy=np.round(100*np.max(cross_val_score(knn, X, y, cv=10)),1) \n",
        "print(Accuracy) \n",
        "#fit knn model \n",
        "knn=knn.fit(Xtrain,ytrain) \n",
        "# predict y values \n",
        "ypred=knn.predict(Xtest) \n",
        "#initialize number of true positives \n",
        "TP=0 \n",
        "#initialize number of false positives \n",
        "FP=0 \n",
        "#initialize number of true negatives \n",
        "TN=0 \n",
        "#initialize number of false negatives \n",
        "FN=0 \n",
        "# count number true positves, true negatives, false positives, and false negatives \n",
        "for i in range(len(ypred)): \n",
        "    if ypred[i]==1 and ytest.iloc[i]==1: \n",
        "        TP+=1 \n",
        "    elif ypred[i]==1 and ytest.iloc[i]==0: \n",
        "        FP+=1 \n",
        "    elif ypred[i]==0 and ytest.iloc[i]==1: \n",
        "        FN+=1 \n",
        "    elif ypred[i]==0 and ytest.iloc[i]==0: \n",
        "        TN+=1 \n",
        "#Calculate true positive rate \n",
        "TPR=TP/(TP+FN) \n",
        "#Calculate precision \n",
        "Precision=TP/(TP+FP) \n",
        "#calculate false positive rate \n",
        "FPR=FP/(TN+FP) \n",
        "#calculate true negative rate \n",
        "TNR=TN/(TN+FP) \n",
        "#calculate false negative rate \n",
        "FNR=FN/(TP+FN) \n",
        "#calculate prevalence \n",
        "Prevalence=(TP+FN)/len(ytest) \n",
        "#intialize list \n",
        "c=[] \n",
        "#_r_e_p_l_a_c_e_ _1_ _a_n_d_ _0_ _w_i_t_h_ _‘b_’ _a_n_d_ _‘g_’ _\n",
        "for e in y: \n",
        "    if e==1: \n",
        "        c.append('b') \n",
        "    else: \n",
        "        c.append('g') \n",
        "#create figure \n",
        "plt.figure(1) \n",
        "#display scatter plot \n",
        "plt.scatter(X[:,0],X[:,1],c=c) \n",
        "#display title \n",
        "plt.title('Data after dimensionality reduction') \n",
        "#create figure \n",
        "plt.figure(2) \n",
        "#create points for comparison with (TNR,FPR) \n",
        "x=np.linspace(0,1,100) \n",
        "#make plot \n",
        "plt.plot(x,x,color=(.6,.2,.7)) \n",
        "#make scatter plot \n",
        "plt.scatter(FPR,TPR,marker='d',color=(0,1,.6)) \n",
        "#label y axis \n",
        "plt.ylabel('TPR') \n",
        "#label x axis \n",
        "plt.xlabel('FPR') \n",
        "#title plot \n",
        "plt.title('TPR vs FPR') \n",
        "#make plot \n",
        "plt.figure(3) \n",
        "#display confusion matrix \n",
        "confusion_matrix = pd.crosstab(ytest, ypred, rownames=['Actual'], \n",
        "colnames=['Predicted']) \n",
        "sn.heatmap(confusion_matrix, annot=True) \n",
        "#titlw confusion matrix \n",
        "plt.title('Confusion matrix') \n",
        "# import relevant items \n",
        "import sklearn.model_selection \n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "from sklearn.cluster import KMeans \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "from sklearn.decomposition import PCA \n",
        "#make copy of data frame \n",
        "Z=df.copy().sample(frac=1,random_state=0) \n",
        "#fit MaxMinScaler \n",
        "norm = MinMaxScaler().fit(Z) \n",
        "#transform the data \n",
        "Z = norm.transform(Z) \n",
        "#create new data frame \n",
        "Z=pd.DataFrame(Z,index=df.index,columns=df.columns) \n",
        "#same \n",
        "X=Z.loc[:,'fea_1':'fea_11'] \n",
        "#same \n",
        "y=Z['label'] \n",
        "#initialze pca object \n",
        "pca=PCA(n_components=2) \n",
        "#fit to data \n",
        "pca.fit(X) \n",
        "#perform dimensionality reduction on data \n",
        "X=pca.transform(X) \n",
        "#create data split \n",
        "Xtrain, Xtest, ytrain, ytest = sklearn.model_selection.train_test_split(X, y, train_size=0.95,random_state=1) \n",
        "#initialize cluster centers \n",
        "init=[[-.4,.2],[.2,.2],[.7,.2]] \n",
        "#initialize kmeans \n",
        "kmeans = KMeans(n_clusters=3, init=np.array(init)) \n",
        "#fit it to data \n",
        "kmeans=kmeans.fit(Xtrain) \n",
        "#same \n",
        "colors=[] \n",
        "for e in kmeans.labels_: \n",
        "    if e ==0: \n",
        "        colors.append('m') \n",
        "    elif e==1: \n",
        "        colors.append('g') \n",
        "    elif e==2: \n",
        "        colors.append('b') \n",
        "#Predict cluster \n",
        "ypred=kmeans.predict(Xtest) \n",
        "#same \n",
        "ctest=[] \n",
        "for e in ypred: \n",
        "    if e ==0: \n",
        "        ctest.append('m') \n",
        "    elif e==1: \n",
        "        ctest.append('g') \n",
        "    elif e==2: \n",
        "        ctest.append('b') \n",
        "#Fit \n",
        "plt.scatter(Xtrain[:,0], Xtrain[:,1], c=colors, alpha=0.15) \n",
        "#Test \n",
        "plt.scatter(Xtest[:,0],Xtest[:,1],c=ctest,marker='X',s=500,alpha=.2) \n",
        "#Cluster centers \n",
        "plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=[50,50,50],marker='d',c='k') \n",
        "#same \n",
        "plt.title('Visualization of clustered data') \n",
        "#import relevant items \n",
        "import sklearn \n",
        "import numpy as np \n",
        "import seaborn as sn \n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "#same \n",
        "Z=df.copy().sample(frac=1,random_state=0) \n",
        "#same \n",
        "X=Z.loc[:,'fea_1':'fea_11'] \n",
        "#same \n",
        "y=Z['label'] \n",
        "#initialize decision tree \n",
        "dt = DecisionTreeClassifier(random_state=0) \n",
        "#same \n",
        "Xtrain, Xtest, ytrain, ytest = sklearn.model_selection.train_test_split(X, y, train_size=0.8,random_state=0) \n",
        "#same \n",
        "Accuracy=np.round(100*np.max(cross_val_score(dt, X, y, cv=10)),1) \n",
        "#same \n",
        "model=dt.fit(Xtrain,ytrain) \n",
        "#same \n",
        "ypred=model.predict(Xtest) \n",
        "#same \n",
        "TP=0 \n",
        "FP=0 \n",
        "TN=0 \n",
        "FN=0 \n",
        "#same \n",
        "for i in range(len(ypred)): \n",
        "    if ypred[i]==1 and ytest.iloc[i]==1: \n",
        "        TP+=1 \n",
        "    elif ypred[i]==1 and ytest.iloc[i]==0: \n",
        "        FP+=1 \n",
        "    elif ypred[i]==0 and ytest.iloc[i]==1: \n",
        "        FN+=1 \n",
        "    elif ypred[i]==0 and ytest.iloc[i]==0: \n",
        "        TN+=1  \n",
        "#same \n",
        "TPR=TP/(TP+FN) \n",
        "Precision=TP/(TP+FP) \n",
        "FPR=FP/(TN+FP) \n",
        "TNR=TN/(TN+FP) \n",
        "FNR=FN/(TP+FN) \n",
        "Prevalance=(TP+FN)/len(ytest) \n",
        "#same \n",
        "x=np.linspace(0,1,100) \n",
        "#same \n",
        "plt.figure(1) \n",
        "plt.plot(x,x,color=(.7,.8,.9)) \n",
        "plt.scatter(FPR,TPR,marker='^',color=(.1,1,1)) \n",
        "plt.ylabel('TPR') \n",
        "plt.xlabel('FPR') \n",
        "plt.title('TPR vs FPR') \n",
        "plt.figure(2) \n",
        "#same \n",
        "confusion_matrix = pd.crosstab(ytest, ypred, rownames=['Actual'], colnames=['Predicted']) \n",
        "sn.heatmap(confusion_matrix, annot=True) \n",
        "plt.title('Confusion matrix') \n",
        "#import relevant items \n",
        "import numpy as np \n",
        "from sklearn.metrics import r2_score \n",
        "import matplotlib.pyplot as plt # To visualize \n",
        "import pandas as pd # To read data \n",
        "import sklearn.metrics \n",
        "from sklearn.linear_model import LinearRegression \n",
        "#same \n",
        "Z=df.copy().sample(frac=1,random_state=0) \n",
        "#assign X to new balance column \n",
        "X = Z['new_balance'] # values converts it into a numpy array \n",
        "#assign y to highest balance column \n",
        "y = Z['highest_balance'] # -1 means that calculate the dimension of rows, but have 1 column \n",
        "linear_regressor = LinearRegression() # create object for the class \n",
        "Xtrain, Xtest, ytrain, ytest = sklearn.model_selection.train_test_split(X, y, train_size=0.95,random_state=0) \n",
        "model=linear_regressor.fit(Xtrain.values.reshape(-1, 1), ytrain.values.reshape(-1, 1)) # perform linear regression \n",
        "# get coefficient \n",
        "param=model.coef_[0] \n",
        "#get y-intercept \n",
        "intercept=model.intercept_ \n",
        "#predict \n",
        "ypred = model.predict(Xtest.values.reshape(-1, 1)) # make predictions \n",
        "#determine R2 value \n",
        "r2=r2_score(ytest,ypred) \n",
        "#same \n",
        "plt.scatter(Xtest, ypred,color=(0,1,.9)) \n",
        "#create function that creates a line representing the model on plot \n",
        "def line(s, i): \n",
        "    axes = plt.gca() \n",
        "    x = np.array(axes.get_xlim()) \n",
        "    y = i + s * x \n",
        "    plt.plot(x, y, '-',color=(.2,1,.7)) \n",
        "#call function \n",
        "line(param,intercept) \n",
        "#same \n",
        "plt.xlabel('new balance') \n",
        "#same \n",
        "plt.ylabel('highest balance') \n",
        "#same \n",
        "plt.title('highest balance vs new balance') \n",
        "#import relevant items \n",
        "import sklearn.linear_model \n",
        "import seaborn as sn \n",
        "#create copy of data \n",
        "Z=df.copy().sample(frac=1,random_state=0) \n",
        "#normalize \n",
        "norm = MinMaxScaler().fit(Z) \n",
        "#transform data \n",
        "Z = norm.transform(Z) \n",
        "#make data frame \n",
        "Z=pd.DataFrame(Z,index=df.index,columns=df.columns) \n",
        "#assign x to slice of data frame \n",
        "X=Z.loc[:,'fea_1':'fea_11'] \n",
        "#assign labels to y \n",
        "y=Z['label'] \n",
        "# create data split \n",
        "Xtrain, Xtest, ytrain, ytest = sklearn.model_selection.train_test_split(X, y, train_size=0.75, random_state=0) \n",
        "#initialize logistic regresiion classifier \n",
        "logistic_regression = sklearn.linear_model.LogisticRegression() \n",
        "#calculate accuracy \n",
        "Accuracy=np.round(100*np.max(cross_val_score(logistic_regression, X, y, cv=10)),1) \n",
        "#fit the model \n",
        "model=logistic_regression.fit(Xtrain, ytrain) \n",
        "#make predictions \n",
        "ypred = model.predict(Xtest) \n",
        "#same as before \n",
        "TP=0 \n",
        "FP=0 \n",
        "TN=0 \n",
        "FN=0 \n",
        "#same as before \n",
        "for i in range(len(ypred)): \n",
        "    if ypred[i]==1 and ytest.iloc[i]==1: \n",
        "        TP+=1 \n",
        "    elif ypred[i]==1 and ytest.iloc[i]==0: \n",
        "        FP+=1 \n",
        "    elif ypred[i]==0 and ytest.iloc[i]==1: \n",
        "        FN+=1 \n",
        "    elif ypred[i]==0 and ytest.iloc[i]==0: \n",
        "        TN+=1 \n",
        "#same as before \n",
        "TPR=TP/(TP+FN) \n",
        "Precision=TP/(TP+FP) \n",
        "FPR=FP/(TN+FP) \n",
        "TNR=TN/(TN+FP) \n",
        "FNR=FN/(TP+FN) \n",
        "Prevalance=(TP+FN)/len(ytest) \n",
        "#same as before \n",
        "x=np.linspace(0,1,100) \n",
        "#same as before \n",
        "plt.figure(1) \n",
        "#same as before \n",
        "plt.plot(x,x,color=(.7,.8,.9)) \n",
        "#same as before \n",
        "plt.scatter(FPR,TPR,marker='^',color=(.1,1,1)) \n",
        "#same as before \n",
        "plt.ylabel('TPR') \n",
        "plt.xlabel('FPR') \n",
        "#same as before \n",
        "plt.title('TPR vs FPR') \n",
        "plt.figure(2) \n",
        "confusion_matrix = pd.crosstab(ytest, ypred, rownames=['Actual'], colnames=['Predicted']) \n",
        "#same as before \n",
        "sn.heatmap(confusion_matrix, annot=True) \n",
        "plt.title('Confusion matrix')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}